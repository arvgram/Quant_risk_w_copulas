---
title: "Assignment3"
author: "Anton Linnér & Arvid Gramer"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(evd)
library(SimCop)
library(copula)
```
\section(Theoretical questions)
  \begin{enumerate}
    \item The characterization of the BEVD with unit Fréchet margins ()$G_{*}(x,y) = \mathrm{exp}(-(\frac{1}{x}+\frac{1}{y})A(\frac{x}{x+y}))$ where $A$ is dependence function. For this function it holds that $(G_*(tx,ty))^t = G_*(x,y)$ which makes it max-stable.
    \item The dependence function $A(\omega)$ fulfills the following properties:
    \begin{itemize}
        \item $A(0) = 1$
        \item $A(1) = 1$
        \item $A'(1) \geq 1$
        \item $A'(0) \geq -1$
        \item $A$ is convex
    \end{itemize}
    \item The formula for extreme value copulas is $C(u,v) = (uv)^{A\left(\frac{\log u}{\log uv}\right)}$. Some examples of these are (with $\tilde{u}-\log(u), \tilde{v}=-\log(v)$
    \begin{itemize}
        \item Gumbel copula: $C(u,v) = \exp\left(-(\tilde{u}^\delta +\tilde{v}^\delta)^{1/\delta}\right)$
        \item Galambos copula: $C(u,v) = uv\exp(-(\tilde{u}^{-\delta} +\tilde{v}^{-\delta})^{-1/\delta})$
        \item Hüsler-Reiss copula: $C(u,v) = \exp\left(-\tilde{u}\Phi\left(\frac{1}{\theta} + \frac{\theta}{2}\log(\frac{\tilde{u}}{\tilde{v}})\right)-\tilde{v}\Phi\left(\frac{1}{\theta} + \frac{\theta}{2}\log(\frac{\tilde{u}}{\tilde{v}})\right)\right)$, where $\Phi$ is the standard normal cumulative distribution function. 
    \end{itemize} Any copula that satisfies $C(u,v)^t = C(u^t, v^t)$ is an extreme value copula, and we can thus check that a given copula fulfills this condition to see if it is an extreme value copula.
    \item If we want to create a bivariate extreme value distribution with our choice of arbitrary univariate extreme value margins we can use the extreme value copula and insert $(u, v) = (F_1^{\leftarrow}(x), F_2^{\leftarrow}(y))$, where $F^{\leftarrow}$ is the generalised inverse ($F^{\leftarrow}(t) = \inf\{x: t < F(x)\}$) of the distribution function we want to have as our margins. The distribution  by $C(F_1^{\leftarrow}(x), F_2^{\leftarrow}(y))$ will have the wanted properties.
    \item To derive the formula for Pickand's estimator of the dependence function $A(\omega)$ we use that if $(X,Y) \sim (\mathrm{e}^{-1/x}, \mathrm{e}^{-1/y})$ then
        \begin{equation}
            P\left[\mathrm{min}\left(\frac{1}{(1-\omega)X},\frac{1}{\omega Y}\right)\geq z\right] = P\left[X\leq\frac{1}{(1-\omega)z},Y \leq \frac{1}{\omega z}\right] 
        \end{equation}
        Using that $x, y$ have unit Fréchet distribution we get that the probability is:
        \begin{equation}
            =\mathrm{exp}\left\{ -(z(1-\omega) + \omega z)\mathrm{A}\left(\frac{1/z(1+\omega)}{1/(z(1-\omega))+1/\omega z}\right)\right\} = \mathrm{e}^{-z\mathrm{A}(\omega)}
        \end{equation}
        We can thus estimate $A$ from our data by taking the bivariate samples $(x_i, y_i)$, $i = 1,...,n$ and computing $z_i(\omega) = \mathrm{min}\left(\frac{1}{(1-\omega)x_i},\frac{1}{\omega y_i}\right)$ and since we then have $P(z_i \leq z)$ = $\mathrm{e}^{-z\mathrm{A}(\omega)}$ an estimate of $A$ is \begin{equation}
            \hat{A}(\omega) = \frac{n}{\sum_{i=1}^n(z_i(\omega))}
        \end{equation}
\end{enumerate}
\section(Random number generation from bivariate EVD) \subsection(Parametric bivariate EV models) In the package \texttt{evd} the nine models and their respective dependence and asymmetry parameters are:
\begin{itemize}
    \item Logistic. Dependence parameter $\texttt{r}$ between (0,1]. Smaller \texttt{r} implies higher dependence.
    \item Asymmetrix logistic. Dependence parameter r, as in (symmetric) Logistic. Asymmetry parameters are $\texttt{t}_1$ and $\texttt{t}_2$. Indepence if any of $\texttt{t}_1, \texttt{t}_2$ are 0 or $\texttt{r = 1}$. For complete dependence $\texttt{t}_1= \texttt{t}_2$=1 and $\texttt{r} \rightarrow 0$.
    \item Husler-Reiss. Dependence parameter \texttt{r} $\in (0, \infty)$. Full dependence as $\texttt{r}$ $\rightarrow \infty$, and independence as $\texttt{r} \rightarrow 0$
    \item Negative logistic. Dependence parameter $\texttt{r}$ $>0$. Higher $\texttt{r}$ implies higher dependence. 
    \item Asymmetric negative logistic. Dependence parameter $\texttt{r}>0$ and asymmetry parameters $\texttt{t}_1,\texttt{t}_2$ $\in (0, 1]$. Indepence if any of $\texttt{t}_1,\texttt{t}_2, \texttt{r}$  approaches 0. Complete dependence if $\texttt{t}_1,\texttt{t}_2 = 1,1$ and $\texttt{r} \rightarrow \infty$.
    \item Bilogistic. Parameters $\alpha, \beta$. When $\alpha = \beta$ the model is equivalent to logistic with dependence parameter $\texttt{r} =\alpha$. As in logistic, when $\alpha = \beta =  \texttt{r} \rightarrow 0$ the model tends to complete dependence. Independence as either both tends to 1, or one is fix and other tends to 1.
    \item Negative bilogistic Parameters $\alpha, \beta$. When $\alpha = \beta$ the model is equivalent to negative bilogistic with dependence parameter $\texttt{r} =1\alpha$. When $\alpha = \beta \rightarrow 0$ the model tends to complete dependence. Independence as either both tends to $\infty$, or one is fix and other tends to $\infty$.
    \item Coles-Tawn. Parameters $\alpha, \beta > (0,0)$. As $\alpha = \beta \rightarrow \infty$ the model shows complete dependence. Independence as either both tends to 0, or one is fix and other tends to 0.
    \item Asymmetric mixed distribution. Parameters $\alpha, \beta$ fulfill the following conditions: $\alpha$ and $\alpha + 3 \beta > 0$, and $\alpha + 2\beta$, $\alpha + \beta \leq 1$ As $\beta$ is fix, the strength of dependence increases with $\alpha$. Complete dependence is not achievable. Independence as $\alpha = \beta = 0$.
  \end{itemize}
```{r 2.2 }
par(mfrow=c(2,2))
sim1 = rbvevd(200,dep=0.001, model = "hr") #HR
plot(sim1)
sim2 = rbvevd(200,dep=5, model = "hr") #HR
plot(sim2)

sim3 = rbvevd(200,dep=0.001, asy=c(0.5,1), model = "aneglog") #HR
plot(sim3)
sim4 = rbvevd(200,dep=5,asy=c(0.5,1), model = "aneglog") #HR
plot(sim4)
#Hustler Reiss
(cor(sim1[,1],sim1[,2]))
(cor(sim2[,1],sim2[,2]))

(cor(sim1[,1],sim1[,2], method = "kendall"))
(cor(sim2[,1],sim2[,2], method = "kendall"))

(cor(sim1[,1],sim1[,2], method = "spearman"))
(cor(sim2[,1],sim2[,2], method = "spearman"))
# "A neglog"
(cor(sim3[,1],sim3[,2]))
(cor(sim4[,1],sim4[,2]))

(cor(sim3[,1],sim3[,2], method = "kendall"))
(cor(sim4[,1],sim4[,2], method = "kendall"))

(cor(sim3[,1],sim3[,2], method = "spearman"))
(cor(sim4[,1],sim4[,2], method = "spearman"))

```
\newpage
For the bivariate cases we have:
\begin{itemize}
    \item Bivariate Extreme Value Logistic Copula: dependence parameter $r \geq 1$. 
    \item Bivariate Extreme Value Asymmetric Logistic Copula dependence parameter $r \leq 1$, asymmetry parameters $\theta, \phi \in [0,1]$. If both are $1$ then this is a symmetric logistic.
    \item Bivariate Extreme Value Mixed Model Copula: parameter $0\leq\theta\leq1$
    \item Bivariate Extreme Value Asymmetric Mixed Model Copula: parameters $\theta, \phi$. These fulfill the following conditions, $\theta \geq 0$, $\theta + 3\phi \geq 0$, $\theta+\phi \leq 1$, $\theta + 2\phi \leq 1$.
    \item Bivariate Extreme Value Spline Copula. Parameter: a spline function for the dependence function. 
\end{itemize}
As for the multivariate copulas we have
\begin{itemize}
    \item Multivariate Extreme Value Asymmetric Logistic Copula: $\mathbf{\theta}$, matrix with asymmetry parameters. $\mathbf{r}$, vector with dependency parameters. Terms and conditions apply. 
    \item Multivariate Extreme Value Gumbel Copula. Dependence parameter $r \geq 1$.
    \item Multivariate Clayton Copula. Dependence parameter $\theta > 0$ 
    \item Multivariate Frank Copula. Parameter $\alpha > 0$
\end{itemize}
\subsection{Exercise 2.4}
```{r Exercise 2.4}
par(mfrow=c(3,2))
cop1 = NewBEVAsyLogisticCopula(2, 200, 0.5)
approxcop1 = GetApprox(cop1)
RV1 = GenerateRV(approxcop1, 200)
plot(RV1)

cop1 = NewBEVAsyLogisticCopula(20, 0.5, 0.5)
approxcop1 = GetApprox(cop1)
RV1 = GenerateRV(approxcop1, 200)
plot(RV1)

cop1 = NewBEVAsyLogisticCopula(2, 0.5, 0.5)
approxcop1 = GetApprox(cop1)
RV1 = GenerateRV(approxcop1, 200)
plot(RV1)

cop1 = NewBEVAsyLogisticCopula(2, 1, 0.5)
approxcop1 = GetApprox(cop1)
RV1 = GenerateRV(approxcop1, 200)
plot(RV1)

cop1 = NewBEVAsyLogisticCopula(2, 0.5, 0.5)
approxcop1 = GetApprox(cop1)
RV1 = GenerateRV(approxcop1, 200)
plot(RV1)

cop1 = NewBEVAsyLogisticCopula(2, 0.5, 1)
approxcop1 = GetApprox(cop1)
RV1 = GenerateRV(approxcop1, 200)
plot(RV1)

```
\newpage
\section{Exercise 3}
```{r data}
fre = source("https://www.maths.lth.se/matstat/kurser/fmsn15masm23/datasets/fremantle.R")
port = source("https://www.maths.lth.se/matstat/kurser/fmsn15masm23/datasets/portpirie.R")
```

\subsection{Exercise 3.1}

```{r Exercise 3.1}
fre.years = fre$value$Year
port.years = port$value$Year
```

```{r Exercise 3.2}
tot.years = merge(x = fre$value,y = port$value, by = "Year")
tot.years = tot.years[,-1]
tot.years = tot.years[,-2]
 
```

```{r Exercise 3.3}
library(fitdistrplus)
par(mfrow=c(1,1))
plot(tot.years$SeaLevel.x, tot.years$SeaLevel.y, xlab = "Fremantle max sea level", ylab = "Portpirie max sea level", main="Annual max sea level scatter plot")
```
```{r Exercise 3.4 Parametric}
par(mfrow=c(2,1))
#FML:
#aneglog
ml.fml1 = fbvevd(tot.years, model = c("aneglog"))
#Hr
ml.fml2 = fbvevd(tot.years, model = c("hr"))

#IFM
start1 = list(loc=1, scale = 1, shape = 1)
start2 = list(loc = 3,scale=3,shape = 1)

marg.fit1 = mledist(tot.years[,1], "gev", start = start1)$est
marg.fit2 = mledist(tot.years[,2], "gev", start = start2)$est

#IFM for aneglog
(ml.ifm1 = fbvevd(tot.years, model= c("aneglog"), loc1 = marg.fit1["loc"], scale1 = marg.fit1["scale"], shape1=marg.fit1["shape"],loc2 = marg.fit2["loc"], scale2 = marg.fit2["scale"], shape2=marg.fit2["shape"]))
#IFM for Hr
(ml.ifm2 = fbvevd(tot.years, model= c("hr"), loc1 = marg.fit1["loc"], scale1 = marg.fit1["scale"], shape1=marg.fit1["shape"],loc2 = marg.fit2["loc"], scale2 = marg.fit2["scale"], shape2=marg.fit2["shape"]))

```

```{r 3.5 Non-parametric}
nonpar.gev = abvnonpar(data = tot.years, plot = TRUE)
nonpar.emp = abvnonpar(data = tot.years, plot = TRUE, epmar = TRUE) #Can see that non-empircal transformation shows more indep. 

```

```{r 3.6 Estimates probabilities}
#We have different calcs for A. 

#PARAMETRIC
# a) Asymmetric FML
mar1 = c( ml.fml1$estimate["loc1"],  ml.fml1$estimate["scale1"],  ml.fml1$estimate["shape1"])
mar2 = c( ml.fml1$estimate["loc2"],  ml.fml1$estimate["scale2"],  ml.fml1$estimate["shape2"])
dep =  ml.fml1$estimate["dep"]
asy = c( ml.fml1$estimate["asy1"], ml.fml1$estimate["asy2"])

(prob1 = pbvevd(c(1.7,4.2), dep = dep, asy = asy, mar1=mar1, mar2=mar2, model = c("aneglog"), lower.tail = FALSE)) 

(prob2 = pbvevd(c(1.8,4.4), dep = dep, asy = asy, mar1=mar1, mar2=mar2, model = c("aneglog"), lower.tail = FALSE))

(prob3 = 1 - pbvevd(c(1.478,3.850), dep = dep, asy = asy, mar1=mar1, mar2=mar2, model = c("aneglog")))

mar1 = c( ml.fml2$estimate["loc1"],  ml.fml2$estimate["scale1"],  ml.fml2$estimate["shape1"])
mar2 = c( ml.fml2$estimate["loc2"],  ml.fml2$estimate["scale2"],  ml.fml2$estimate["shape2"])
dep =  ml.fml2$estimate["dep"]
(prob1.sym = pbvevd(c(1.7,4.2), dep = dep, mar1=mar1, mar2=mar2, model = c("hr"), lower.tail = FALSE))
(prob2.sym = pbvevd(c(1.8,4.4), dep = dep, mar1=mar1, mar2=mar2, model = c("hr"), lower.tail = FALSE))
(prob3.sym = 1- pbvevd(c(1.478,3.850), dep = dep, mar1=mar1, mar2=mar2, model = c("hr")))

```


```{r empirical probs}
prob1.emp <- as.numeric(tot.years$SeaLevel.x > 1.7 & tot.years$SeaLevel.y > 4.2)
(prob1.emp = sum(prob1.emp)/length(prob1.emp))

prob2.emp <- as.numeric(tot.years$SeaLevel.x > 1.8 & tot.years$SeaLevel.y > 4.4)
(prob2.emp = sum(prob2.emp)/length(prob2.emp))


prob3.emp <- as.numeric(tot.years$SeaLevel.x >=  1.478 | tot.years$SeaLevel.y >= 3.850)
(prob3.emp = sum(prob3.emp)/length(prob3.emp))
```

```{r non-parametric}
G_star <- function(x,y){
  val = exp(-(1/x + 1/y)*abvnonpar(x=x/(x+y), data=tot.years ))
  return(val)
}

transform <- function(x, mar){
  mar1 = mar
  x.t = c((1+mar1[3]*(x-mar1[1])/mar1[2])^(1/mar1[3]), use.names=FALSE) 
return(x.t)}



#1.7 and 4.2
x.t = transform(1.7, mar1)
y.t = transform(4.2, mar2)
(prob1.nonpar = 1 - pgev(1.7, loc=mar1[1], scale=mar1[2], shape = mar1[3]) - pgev(4.2, loc=mar2[1], scale=mar2[2], shape = mar2[3]) + G_star(x.t, y.t))

#1.8 and 4.4
x.t = transform(1.8,  mar1)
y.t = transform(4.4,  mar2)
(prob2.nonpar = 1 - pgev(1.8, loc=mar1[1], scale=mar1[2], shape = mar1[3]) - pgev(4.4, loc=mar2[1], scale=mar2[2], shape = mar2[3]) + G_star(x.t, y.t))

#1.478 and 3.850
x.t = transform(1.478, mar1)
y.t = transform(3.850,  mar2)
(prob3.nonpar = 1 - G_star(x.t, y.t))
```


```{r 3.7}

#By using bayes relation betweenn conditional probabilities we get:
#Non.par
mar1 = mledist(tot.years[,1], "gev", start = start1)$est
mar2 = mledist(tot.years[,2], "gev", start = start2)$est
x.t1 = transform(1.478, mar1)
y.t1 = transform(3.850, mar2)
x.t2 = transform(1.95, mar1)
y.t2 = transform(4.8, mar2)
(prob1 = ((1 - G_star(x.t1, y.t1)) - (1 - G_star(x.t2, y.t2)))/ (1 - G_star(x.t1, y.t1)))

#par (symmetric)
x.t1 = transform(1.478, mar1)
y.t1 = transform(3.850, mar2)
x.t2 = transform(1.95, mar1)
y.t2 = transform(4.8, mar2)
(prob1 =  (pbvevd(c(x.t1,y.t1), dep = dep, mar1=mar1, mar2=mar2, model = c("hr"), lower.tail = FALSE) -  (pbvevd(c(x.t2,y.t2), dep = dep, mar1=mar1, mar2=mar2, model = c("hr"), lower.tail = FALSE))/pbvevd(c(x.t1,y.t1), dep = dep, mar1=mar1, mar2=mar2, model = c("hr"), lower.tail = FALSE)))
```

```{r 3.8 plotting}
par(mfrow=c(2,2))
ml.fml2 = fbvevd(tot.years, model = c("hr"))
plot(ml.fml2, which=5, p=c(0.75, 0.9, 0.95))
qcbvnonpar(data = tot.years, plot=TRUE, p = c(0.75, 0.9, 0.95))
qcbvnonpar(data = tot.years, plot=TRUE, epmar = TRUE)
```

```{r 3.9 }
tot.years = merge(x = fre$value,y = port$value, by = "Year")


# min and max
minv <- min(tot.years["Year"])
maxv <- max(tot.years["Year"])

# Normalize to [-1, 1]
tot.years["Year"] <- ((tot.years["Year"] - minv) / (maxv - minv)) * 2 - 1

x_dat = tot.years[-1]
x_dat = x_dat[-2]

#Test bitg
ml.fml1.2 = fbvevd(x_dat, model = c("aneglog"),nsloc1 =tot.years["Year"],nsloc2 =tot.years["SOI"])
#Hr
ml.fml2.2 = fbvevd(x_dat, model = c("hr"), nsloc1 =tot.years["Year"],nsloc2 =tot.years["SOI"])
anova(ml.fml1.2, ml.fml1)

(A <- logLik(ml.fml1))
(B<- logLik(ml.fml1.2))

(stat <- -2 * (as.numeric(A)-as.numeric(B)))
(p.val <- pchisq(stat, df = 2, lower.tail = FALSE)) #under 0.05 we should use the complex model ( reject null)


(A <- logLik(ml.fml2))
(B<- logLik(ml.fml2.2))

(stat <- -2 * (as.numeric(A)-as.numeric(B)))
(p.val <- pchisq(stat, df = 2, lower.tail = FALSE)) #under 0.05 we should use the complex model ( reject null)

#Test "YEAR" ----------------------------

ml.fml1.2 = fbvevd(x_dat, model = c("aneglog"),nsloc1 =tot.years["Year"])
#Hr
ml.fml2.2 = fbvevd(x_dat, model = c("hr"), nsloc1 =tot.years["Year"])

(A <- logLik(ml.fml1))
(B<- logLik(ml.fml1.2))

(stat <- -2 * (as.numeric(A)-as.numeric(B)))
(p.val <- pchisq(stat, df = 1, lower.tail = FALSE)) #under 0.05 we should use the complex model ( reject null)


(A <- logLik(ml.fml2))
(B<- logLik(ml.fml2.2))

(stat <- -2 * (as.numeric(A)-as.numeric(B)))
(p.val <- pchisq(stat, df = 1, lower.tail = FALSE)) #under 0.05 we should use the complex model ( reject null)



#Test SOI ---------------------------------------
ml.fml1.2 = fbvevd(x_dat, model = c("aneglog"),nsloc1 =tot.years["SOI"])
#Hr
ml.fml2.2 = fbvevd(x_dat, model = c("hr"), nsloc1 =tot.years["SOI"])

(A <- logLik(ml.fml1))
(B<- logLik(ml.fml1.2))

(stat <- -2 * (as.numeric(A)-as.numeric(B)))
(p.val <- pchisq(stat, df = 1, lower.tail = FALSE)) #under 0.05 we should use the complex model ( reject null)


(A <- logLik(ml.fml2))
(B<- logLik(ml.fml2.2))

(stat <- -2 * (as.numeric(A)-as.numeric(B)))
(p.val <- pchisq(stat, df = 1, lower.tail = FALSE))


#Conclusion, SOI is statisctically significant!
```
```{r Exercise 4 a}
tot.years  = as.matrix(tot.years)
start1 = list(loc=1, scale = 1, shape = 1)
start2 = list(loc = 3,scale=3,shape = 2)
marg.fit1 = mledist(tot.years[,1], "gev", start = start1)$est
marg.fit2 = mledist(tot.years[,2], "gev", start = start2)$est

#From data we can see that tau is about 0.2
tau = 0.2

#Copulas
gumbel.cop <- gumbelCopula(iTau(gumbelCopula(), tau))
galambos.cop <- galambosCopula (iTau(galambosCopula(), tau))
hr.cop <-huslerReissCopula(iTau(huslerReissCopula(), tau))

mvdc1 =  mvdc(gumbel.cop, margins=c("gev", "gev"), paramMargins=list(list(loc=1,scale=0.2, shape = -0.1), list(loc=3.8,scale=0.2, shape = -0.1)))
mvdc2 =  mvdc(galambos.cop, margins=c("gev", "gev"), paramMargins=list(list(loc=1,scale=0.2, shape = -0.1), list(loc=3.8,scale=0.2, shape = -0.1)))
mvdc3 =  mvdc(hr.cop, margins=c("gev", "gev"), paramMargins=list(list(loc=1,scale=0.2, shape = -0.1), list(loc=3.8,scale=0.2, shape = -0.1)))

#fml
start = c(1,2,1,2,1,1,2)
fit1a = fitMvdc(tot.years, mvdc1, start = start, optim.control=list(trace=TRUE, maxit = 2000))
fit2a = fitMvdc(tot.years, mvdc2, start = start, optim.control=list(trace=TRUE, maxit = 2000))
fit3a= fitMvdc(tot.years, mvdc3, start = start, optim.control=list(trace=TRUE, maxit = 2000))

#IFM
#Use margins to get the obs to "probabilities"

udat <- cbind(pgev(tot.years[, 1], loc = marg.fit1[1], scale=marg.fit1[2],shape = marg.fit1[3]), pgev(tot.years[, 2], loc = marg.fit2[1], scale=marg.fit2[2],shape = marg.fit2[3]))
fit1b <- fitCopula(data=udat, mvdc1@copula, start = 2)
fit2b <- fitCopula(data=udat, mvdc2@copula, start = 2)
fit3b <- fitCopula(data=udat, mvdc3@copula, start = 2)

#CML
n=length(tot.years[,1])
eu <- cbind((rank(tot.years[, 1]) - 0.5)/n, (rank(tot.years[, 2]) - 0.5)/n)

fit1c <- fitCopula(data = eu, mvdc1@copula, start = 2)
fit2c <- fitCopula(data = eu, mvdc2@copula, start = 2)
fit3c <- fitCopula(data= eu, mvdc3@copula, start = 0.2)


#Compare with evd ?!

```


```{r exercise 4.3}
gof1 = gofEVCopula(mvdc1@copula, as.matrix(tot.years),optim.method="Nelder-Mead")

gof2 = gofEVCopula(mvdc2@copula, as.matrix(tot.years),optim.method="Nelder-Mead")

gof3 = gofEVCopula(mvdc3@copula, x=as.matrix(tot.years),optim.method="Nelder-Mead")


```

```{r exercise 4.4}
w = seq(0,1, by=0.01)
cfg = An.biv(tot.years, w, estimator=c("CFG"))
pickands =  An.biv(tot.years, w, estimator=c("Pickands"))

par(mfrow=c(2,1))
plot(cfg, main="cfg")
plot(pickands, main = "Pickands")
```

```{r Exercise 4.5}


```


\newline
\section{Exercise 5}

We see that we don't force convexity the result plot is not convex. How ever, enforcing convexity only leaves 13 points withing the hull.

```{r exercise 5.1}
A1 = NonparEstDepFct(tot.years$SeaLevel.x, y=tot.years$SeaLevel.y)
A2 = NonparEstDepFct(tot.years$SeaLevel.x, y=tot.years$SeaLevel.y, convex.hull = FALSE )
par(mfrow=c(2,1))
plot(A1)
plot(A2)
```

```{r Exercise 5.2}
## Data from Hall and Tajvidi (2004, ANZJS)
Estim1 <- NonparEstDepFct(x=tot.years$SeaLevel.x, y= tot.years$SeaLevel.y, convex.hull = FALSE)

## Plot modified Pickands Function and area in which
## dependence function must lie
plot(Estim1, ylim = c(0.5,1), xlab = "w", ylab = "A(w)", type="l", lty="longdash")
polygon(c(0, 0.5, 1, 0), c(1, 0.5, 1, 1))

## Fit spline to Pickands function and add to plot
spline.fit <- SplineFitDepFct(Estim1)
curve(spline.fit, n = 301, add = TRUE, lty = "dashed")
```

```{r Exercise 5.3}

spline.cop = NewBEVSplineCopula(spline.fit)
spline.cop.approx = GetApprox(spline.cop)
plot(spline.cop.approx)

```

```{r Exercise 5.4}
RVs = GenerateRV(spline.cop.approx, 10000)
#a)
quant1 = ecdf(tot.years[,1])(1.7) 
quant2 = ecdf(tot.years[,2])(4.2) 

prob1.emp <- as.numeric(RVs[,1] > quant1 & RVs[,2] > quant2)
(prob1.emp = sum(prob1.emp)/length(prob1.emp))
#b)
quant1 = ecdf(tot.years[,1])(1.8) 
quant2 = ecdf(tot.years[,2])(4.4) 

prob1.emp <- as.numeric(RVs[,1] > quant1 & RVs[,2] > quant2)
(prob1.emp = sum(prob1.emp)/length(prob1.emp))
#C)
quant1 = ecdf(tot.years[,1])(1.478) 
quant2 = ecdf(tot.years[,2])(3.85) 

prob1.emp <- as.numeric(RVs[,1] > quant1  |  RVs[,2] > quant2)
(prob1.emp = sum(prob1.emp)/length(prob1.emp))


#d) ???
#prob.fin = (prob1.emp - prob2.emp)/prob1.emp

prob2.emp <- as.numeric(RVs[,1] >= 1  |  RVs[,2] >= 1)
(prob2.emp = sum(prob2.emp)/length(prob2.emp))


quant3 = ecdf(tot.years[,1])(1.95) 
quant4 = ecdf(tot.years[,2])(4.8)
 


```

```{r Exercise 6 Arvid}
#1 thresholds
u1 = quantile(tot.years[,1], probs = 0.30)
u2 = quantile(tot.years[,2], probs = 0.30)

#2 fit models
pot1 <- fbvpot(tot.years,c(u1,u2), model = "hr")
pot2 <- fbvpot(tot.years, c(u1,u2), model = "aneglog", std.err = FALSE)

#3 plot
par(mfrow = c(1,2))
plot(pot1, which = 3, p=c(0.75,0.9,0.95), xlab="Fremantle",ylab="Port Pirie")
plot(pot2, which = 3, p=c(0.75,0.9,0.95), xlab="Fremantle",ylab="Port Pirie")

#4 calculate
# we seek prob = P((X,Y) < (1.95,4.8)|(X,Y) < ̸ (1.478, 3.850)
# P(A|B) = P(A & B)/P(B) = P((1.478, 3.850) < (X,Y) < (1.95, 4.8))/P((1.478, 3.850) < (X,Y))
# = (F(1.95, 4.8)-F(1.478, 3.850))/(1-F(1.478, 3.850))

gamx <- abs(pot1$estimate[2]) # gamma 
sigx <- pot1$estimate[1] # sigma
gamy <- abs(pot1$estimate[4]) # gamma
sigy <- pot1$estimate[3] # sigma

etax <- sum(tot.years[,1]>u1)/length(tot.years[,1]) # eta
etay <- sum(tot.years[,2]>u2)/length(tot.years[,2]) # eta


# transform to fit margins
x_tilde <- -log(1-etax*(1+gamx*(1.95-u1)/sigx)^(-1/gamx))^-1
y_tilde <- -log(1-etay*(1+gamy*(4.80-u2)/sigy)^(-1/gamy))^-1

p1 <- pbvevd(
  q=c(x_tilde,y_tilde),
  dep=pot1$estimate[5],
  model="hr",
  lower.tail=TRUE)

# transform to fit margins
x_tilde <- -log(1-etax*(1+sigx*(1.478-u1)/sigx)^(-1/gamx))^-1
y_tilde <- -log(1-etay*(1+sigy*(3.850-u2)/sigy)^(-1/gamy))^-1
p2 <- pbvevd(q = c(x_tilde, y_tilde), dep = pot1$estimate[5], model = "hr", lower.tail = TRUE)

prob1 <- (p1-p2)/(1-p2)
print(prob1)


# test P(2,5...)-thing
x_tilde <- -log(1-etax*(1+gamx*(2-u1)/sigx)^(-1/gamx))^-1
y_tilde <- -log(1-etay*(1+gamy*(5-u2)/sigy)^(-1/gamy))^-1

p1 <- pbvevd(
  q=c(x_tilde,y_tilde),
  dep=pot1$estimate[5],
  model="hr",
  lower.tail=TRUE)

x_tilde <- -log(1-etax*(1+sigx*(1.478-u1)/sigx)^(-1/gamx))^-1
y_tilde <- -log(1-etay*(1+sigy*(3.850-u2)/sigy)^(-1/gamy))^-1
p2 <- pbvevd(q = c(x_tilde, y_tilde), dep = pot1$estimate[5], model = "hr", lower.tail = TRUE)

prob2 <- (p1-p2)/(1-p2)
print(prob2)


## type 2
#mgpd package in R (authored by Pal Rakonczai).
library(mgpd)
par(mfrow=c(1,1))
# The following is sea level data for Fremantle and Portpirie
fp <-
  structure(c(1.49, 1.46, 1.34, 1.74, 1.62, 1.46, 1.71, 1.74, 1.55,
              1.43, 1.62, 1.49, 1.58, 1.34, 1.37, 1.62, 1.31, 1.43, 1.49, 1.55,
              1.71, 1.49, 1.46, 1.52, 1.58, 1.65, 1.49, 1.52, 1.52, 1.49, 1.62,
              1.86, 1.58, 1.62, 1.46, 1.43, 1.46, 1.62, 1.68, 1.83, 1.62, 1.46,
              1.58, 1.77, 1.62, 1.71, 1.46, 1.6, 1.5, 1.6, 1.9, 1.7, 1.4, 1.8,
              1.37, 1.46, 1.61, 1.43, 1.67, 1.62, 1.57, 1.56, 1.46, 4.03, 3.83,
              3.65, 4.01, 4.08, 4.18, 3.8, 4.36, 3.96, 3.98, 4.69, 3.85, 3.96,
              3.85, 3.93, 3.75, 3.63, 3.57, 3.97, 4.05, 4.24, 4.22, 3.73, 4.37,
              4.06, 3.71, 3.96, 4.06, 4.55, 3.79, 3.89, 4.11, 3.85, 3.86, 3.86,
              4.21, 4.01, 4.11, 4.24, 3.96, 4.21, 3.74, 3.85, 3.88, 3.66, 4.11,
              3.71, 4.18, 3.9, 3.78, 3.91, 3.72, 4, 3.66, 3.62, 4.33, 4.55,
              3.75, 4.08, 3.9, 3.88, 3.94, 4.33), .Dim = c(63L, 2L))


# we must give the threshold levels first
thr = apply(fp,2,quantile,prob=0.3)
nms = c("Fremantle","Port Pirie")
# here we convert the data to the desired form
potdata = mgpd_data(fp, thr=thr)
plot(potdata[,1], potdata[,2], xlab=nms[1], ylab=nms[2], main="Exceedances" )
abline(h=0, v=0 , lty=2)

init = mgpd_init(potdata)
est.log = fbgpd(c(0,1,-0.1,0,1,-0.1,1.2), dat=potdata, model="log",
                 fixed=FALSE )
print(est.log$par)
mar1 = est.log$par[1:3]
mar2 = est.log$par[4:6]
dep = est.log$par[7]

#probability


p1.log <- pbgpd(1.95-1.478,
                4.8-3.850,
                model="log",
                mar1=mar1,
                mar2=mar2,
                dep=dep
                )

p2.log <- pbgpd(0,
                0,
                model="log",
                mar1=mar1,
                mar2=mar2,
                dep=dep
                )

prob1 <- (p1.log - p2.log)/(1 - p2.log)

## I could not find documentation on this package so I will use same Tajvidi's as implemented in the example
est.taj = fbgpd(
  c(est.log$par[1:7],0.1), 
  dat=potdata, 
  model="taj",
  fixed=FALSE)


# plotting
z = outer(x,
          y,
          pbgpd,
          model="log",
          mar1=est.log$par[1:3],
          mar2=est.log$par[4:6], 
          dep=est.log$par[7]
          )
image.plot(x+thr[1], y+thr[2], z)
contour(x+thr[1], y+thr[2], z,add=T)

```
# Type 2 BGPD



```

```{r Exercise 6 type 1}
#1
# u1 = quantile(tot.years[,1], probs = 0.30)
# u2 = quantile(tot.years[,2], probs = 0.30)
# #2
# mod.sym = fbvpot(tot.years, threshold = c(u1,u2), model = c("log"), std.err = FALSE)
# mod.asym =  fbvpot(tot.years, threshold = c(u1,u2), model = c("aneglog"), std.err=FALSE)
# #3
# plot(mod.sym, p=c(0.75,0.9,0.95),xlab="",ylab="")
# plot(mod.asym, p=c(0.75,0.9,0.95), which = 3)
# # # 4
# # marg1 = fpot(tot.years$SeaLevel.x, u1)$estimate
# marg2 = fpot(tot.years$SeaLevel.y, u2)$estimate
# 
# x.lar = sum(tot.years$SeaLevel.x > u1)/length(tot.years$SeaLevel.x)
# 
# y.lar = sum(tot.years$SeaLevel.y > u2)/length(tot.years$SeaLevel.y)
# 
# x.t = -(log(1-x.lar*(1+marg1[2]*(tot.years$SeaLevel.x - u1)/marg1[1])^(-1/marg1[2])))^(-1)
# 
# y.t = -(log(1-y.lar*(1+marg2[2]*(tot.years$SeaLevel.y - u2)/marg2[1])^(-1/marg2[2])))^(-1)
# 
# mod.sym = fbvpot(c(x.t,y.t), threshold = c(u1,u2), model = c("log"))
# mod.asym =  fbvpot(tot.years, threshold = c(u1,u2), model = c("aneglog"), std.err=FALSE)




#Very similar, kinda agree with 3.8
```
```{r Exercise 6 type 1 test2}
#1
# u1 = quantile(tot.years[,1], probs = 0.30)
# u2 = quantile(tot.years[,2], probs = 0.30)
# #2
# mod.sym = fbvpot(tot.years, threshold = c(u1,u2), model = c("log"))
# mod.asym =  fbvpot(tot.years, threshold = c(u1,u2), model = c("aneglog"), std.err=FALSE)
# #3
# plot(mod.sym, p=c(0.75,0.9,0.95), which = 3)
# plot(mod.asym, p=c(0.75,0.9,0.95), which = 3)
# # 4
# marg1 = fpot(tot.years$SeaLevel.x, u1)$estimate
# marg2 = fpot(tot.years$SeaLevel.y, u2)$estimate
# 
# x.lar = sum(tot.years$SeaLevel.x > u1)/length(tot.years$SeaLevel.x)
# 
# y.lar = sum(tot.years$SeaLevel.y > u2)/length(tot.years$SeaLevel.y)
# 
# x.t = -(log(1-x.lar*(1+marg1[2]*(tot.years$SeaLevel.x - u1)/marg1[1])^(-1/marg1[2])))^(-1)
# 
# y.t = -(log(1-y.lar*(1+marg2[2]*(tot.years$SeaLevel.y - u2)/marg2[1])^(-1/marg2[2])))^(-1)
# 
# tot.years.copy = tot.years
# tot.years.copy$SeaLevel.x = x.t
# tot.years.copy$SeaLevel.y = y.t
# mod.sym = fbvpot(c(x.t,y.t), threshold = c(u1,u2), model = c("hr"))
# mod.asym =  fbvpot(tot.years, threshold = c(u1,u2), model = c("aneglog"), std.err=FALSE)
 





#Very similar, kinda agree with 3.8
```





```{r exercise 6 type 2}
library(mgpd)
fp <-
structure(c(1.49, 1.46, 1.34, 1.74, 1.62, 1.46, 1.71, 1.74, 1.55,
1.43, 1.62, 1.49, 1.58, 1.34, 1.37, 1.62, 1.31, 1.43, 1.49, 1.55,
1.71, 1.49, 1.46, 1.52, 1.58, 1.65, 1.49, 1.52, 1.52, 1.49, 1.62,
1.86, 1.58, 1.62, 1.46, 1.43, 1.46, 1.62, 1.68, 1.83, 1.62, 1.46,
1.58, 1.77, 1.62, 1.71, 1.46, 1.6, 1.5, 1.6, 1.9, 1.7, 1.4, 1.8,
1.37, 1.46, 1.61, 1.43, 1.67, 1.62, 1.57, 1.56, 1.46, 4.03, 3.83,
3.65, 4.01, 4.08, 4.18, 3.8, 4.36, 3.96, 3.98, 4.69, 3.85, 3.96,
3.85, 3.93, 3.75, 3.63, 3.57, 3.97, 4.05, 4.24, 4.22, 3.73, 4.37,
4.06, 3.71, 3.96, 4.06, 4.55, 3.79, 3.89, 4.11, 3.85, 3.86, 3.86,
4.21, 4.01, 4.11, 4.24, 3.96, 4.21, 3.74, 3.85, 3.88, 3.66, 4.11,
3.71, 4.18, 3.9, 3.78, 3.91, 3.72, 4, 3.66, 3.62, 4.33, 4.55,
3.75, 4.08, 3.9, 3.88, 3.94, 4.33), .Dim = c(63L, 2L))

thr= apply(fp,2,quantile,prob=0.3)
nms = c("Fremantle","Portpirie")
# here we convert the data to the desired form
potdata = mgpd_data( fp, thr=thr)
plot( potdata[,1], potdata[,2], xlab=nms[1], ylab=nms[2], main="Exceedances" )
abline( h=0, v=0 , lty=2)


#fitting logistic model, (here you can also try other models; see the
#help page for bgpd_fit).
#init= mgpd_init(potdata)
# est.log       = bgpd_fit(c(init,1.2),dat=potdata,model="log",fixed=FALSE)
# default initial value (above) doesn't work so we set them manually:
est.log = fbgpd( c(0,1,-0.1,0,1,-0.1,1.2), dat=potdata, model="log", fixed=FALSE )
est.log$par
# domain and probabilities for the pred regs - PRECISION COULD BE BETTER 0.001 or even finer
x  = seq(-0.22,1,  0.005)
y = seq(-0.27,1,  0.005)
Q = c(0.95,0.9,0.75)
# evaluating the density over the grid
z  = outer(  x,  y,  dbgpd,  model="log", mar1 = est.log$par[1:3], mar2 = est.log$par[4:6], dep = est.log$par[7] )
# calculating the prediction region
reg     = dbgpd_region( x,  y,  z, quant = Q )
# plotting on the original scale (threshold levels are added back)

contour(reg$x+thr[1], reg$y+thr[2], reg$z, levels=reg$q, drawlabels=FALSE,
main="Logistic BGPD", col=c(1,3,4),
xlab=paste(nms[1],"(m)"),ylab=paste(nms[2],"(m)"),lwd=1)
abline( h=thr[2], v=thr[1], lty=2 )
legend( "bottomright", c(expression(gamma==0.95), expression(gamma==0.9),
expression(gamma==0.75)), lty=1, col=c(1,3,4), title="Regions", bty="n")
# and the obs
points( potdata[,1]+thr[1],potdata[,2]+thr[2],cex=0.7)
#mvdc fmvdc

#Try model neglog

est.neglog = fbgpd(initpar=c(0, 0.1, 0.5, 0.2, 0.3, 0.7, 2.5), dat=potdata, model="neglog", fixed=FALSE)

## Calculating probabilities: note that we need to subtract the thresholds
prob.neglog <- pbgpd(1.95 - 1.478,4.8 -
3.850,model="log",mar1=est.neglog$par[1:3],mar2=est.neglog$par[4:6],dep=est.neglog$par[7])


# TESTING OTHER DISTRIBUTION FUNCTIONS
#neglog modell
z = outer( x, y, pbgpd, model="neglog", mar1 = est.neglog$par[1:3], mar2 =
est.neglog$par[4:6], dep = 1.5 )
#image.plot(x+thr[1], y+thr[2], z)
contour(x+thr[1], y+thr[2], z,add=T)
abline( h=thr[2], v=thr[1], lty=2 )
```

















