---
output:
  pdf_document: default
  html_document: default
---
---
title: "Assignment 2"
author: "Arvid Gramer and Anton Linnér"
date: "`r Sys.Date()`"
output: pdf_document
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
set.seed(2) 
library(copula)
library(scatterplot3d)
library(ggplot2)
library(reshape2)
```
\newpage
\section{Exercise 1}
\subsection{}
    An Archimedean copula is a copula on the form: \\ $C(u_1,..., u_n) =       \varphi^{-1}(\sum^n_{i=1}\varphi(u_i))$, where $\varphi$ is the copula's generator function. The conditions for this generator function is that \begin{itemize}
        \item $\varphi(0) = 1,$ $\varphi(\infty) = 0$
        \item $\varphi$ is decreasing
        \item $\varphi$ is continuous
    \end{itemize} 
    A convenient way of obtaining such generator functions is to take the Laplace transform of a random variable, $\varphi_X(t) = \int_0^{\infty}\mathrm{e}^{-tx}f_X(x)dx$. Depending on choice of generator function we retrieve different families of Archimedean copulas, such as Clayton, Gumbel or Frank copulas.
\subsection{}
    \begin{itemize}
        \item Frank copula: This copulas has generator function $\varphi(t) = \mathrm{ln}\left(\frac{\mathrm{e}^{-\theta t}-1}{\mathrm{e}^{-\theta}-1}\right)$, C(u,v) = $\varphi^{-1}(\varphi(u) + \varphi(v)) = \frac{1}{\theta}\mathrm{ln}\left(1+\frac{(\mathrm{e}^{-\theta u}-1)(\mathrm{e}^{-\theta v}-1)}{\mathrm{e}^{-\theta}-1}\right)$
    
        \item Clayton copula: The generator is $\varphi(t) = t^{-\theta}-1$, which results in a copula $C(u,v) = (u^{-\theta}+v^{-\theta}-1)^{-1/\theta}$
        \item Gumbel copula: has generator $\varphi(t)=(-\mathrm{log}(t))^{\theta}$ which generates a copula: $C(u,v) = \mathrm{exp}\left(-\left((-\mathrm{log}(u))^{\theta} + (-\mathrm{log}(v))^{\theta}\right) \right)$
        \item The Ali-Mikhail-Haq copula has a generator $\varphi (t) = \mathrm{log}\left(\frac{1-\theta(1-t)}{t}\right)$ which yields a copula $C(u,v) = \frac{uv}{1-\theta(1-u)(1-v)}$.
    \end{itemize}
    
\subsection{} 
The laplace transform of a random variable with distribution function $F(x)$ and density $f(x)$ is defined as (in the continuous case):

\begin{equation}
    \int_0^\infty\mathrm{e}^{-tx}\mathrm{d}F(x)  = \int_0^\infty\mathrm{e}^{-tx}f(x)\mathrm{d}x 
\end{equation}
For a Gamma-distributed variable this becomes:
\begin{equation}
    \int_0^\infty\mathrm{e}^{-tx}\mathrm{e}^{-bx}\frac{b^a}{\Gamma(a)}x^{a-1}\mathrm{d}x 
\end{equation}
With the gamma function being defined as $\Gamma(a) = \int_0^{\infty}x^{a-1}\mathrm{e}^{-x}\mathrm{d}x$ we will have some issues with it unless we can get rid of it somehow. We try to squeeze it into the expression by:
\begin{equation}
    \int_0^\infty\mathrm{e}^{-x(t+b)}\frac{b^a}{\Gamma(a)}x^{a-1}\mathrm{d}x = \frac{b^a}{\Gamma(a)(t+b)^a}\int_0^\infty\mathrm{e}^{-(t+b)x}((t+b)x)^{a-1}(t+b)\mathrm{d}x 
\end{equation}
With a subsitution $y = (t+b)x$ we have $\mathrm{d}y = (t+b)\mathrm{d}x$ which yields
\begin{equation}
    \frac{b^a}{\Gamma(a)(t+b)^a}\int_0^\infty\mathrm{e}^{-y}y^{a-1}\mathrm{d}y = \frac{b^a}{\Gamma(a)(t+b)^a}\Gamma(a) = \frac{b^a}{(t+b)^a}
\end{equation}
With $b = 1, a = 1/\theta$ this becomes $(t+1)^{-1/\theta}$. The inverse, $t^{-\theta}-1$ is a generator for the class of copulas, $C(u,v) = \left(\mathrm{max}\left[u^{-\theta}+v^{-\theta} -1, 0\right]\right)^{-1/\theta}$.

\subsection{}
With \textbf{X} being a random $d$-dimensional random vector, we say that it follows a multivariate normal distribution if it has a probability density function:
\begin{equation}
    f(\mathbf{x};\boldsymbol\mu, \boldsymbol\sigma)=(2\pi)^{-k/2}\det (\boldsymbol\Sigma)^{-1/2} \, \exp \left( -\frac{1}{2} (\mathbf{x} - \boldsymbol\mu)^\mathrm{T} \boldsymbol\Sigma^{-1}(\mathbf{x} - \boldsymbol\mu) \right)
\end{equation}
Where $\boldsymbol\Sigma$ is a covariance matrix describing the variance of the $d$ elements in $\mathbf{X}$ and the covariance between them. $\boldsymbol \mu$ is a vector of their respective means. Furthermore, if it is a multivariate t-distribution it follows
\begin{equation}
    f(\mathbf{x}; \boldsymbol\mu, \tilde{\boldsymbol\Sigma}, \nu) = 
    \frac{\Gamma\left[(\nu+p)/2\right]}{\Gamma(\nu/2)\nu^{p/2}\pi^{p/2}\left|{\tilde{\boldsymbol\Sigma}}\right|^{1/2}}\left[1+\frac{1}{\nu}({\mathbf x}-{\boldsymbol\mu})^T{\tilde{\boldsymbol\Sigma}}^{-1}({\mathbf x}-{\boldsymbol\mu})\right]^{-(\nu+p)/2}
\end{equation}
In this density, $\nu$ is the number of degrees of freedom, $\boldsymbol\mu$ the mean of the elements and $\tilde{\boldsymbol{\Sigma}}$ a scaled covariance matrix $\tilde{\boldsymbol{\Sigma}}=\boldsymbol\Sigma\nu/(\nu-2)$ for $\nu > 2$.

\subsection{}
There are several ways to measure dependence. Some of these are Pearson's $\rho$, Kendall's $\tau$ and Spearman's $\rho_S$.
\begin{itemize}
    \item Pearson's $\rho$ is the linear relationship between two variables. It is calculated as the covariance between two variables scaled by their respective standard deviation. It cannot capture nonlinear dependence and is sensitive to outliers.
    \item Kendall's $\tau$ is a measure that assesses the probability of concordance minus the probability of discordance between two observations in a sample. A concordant pair of observation $(x_1,y_1), (x_2, y_2)$ means that $x_1 > x_2 \rightarrow y_1 > y_2$. A concordant pair means the opposite. This is a more ordinal way of measuring dependence than correlation since it only counts if the ordinal relationship between two samples hold and not how much. This makes it more robust. 
    \item Spearman's $\rho_S$ is the correlation (that is Pearson's $\rho$) but instead calculated on the rank of the variables in each observation in a sample. In a sample $\{(x_i, y_i)\}$ the rank of each $x_i$ and $y_i$ is calculated, namely one assigns the number associated with the place they would have if all $x$  
\end{itemize}

\section{Exercise 2}

\subsection{Exercise 2.1}
We can see that Spearmans Rho and normal correlation are somewhat similar, but Kendalls Tau is lower than both. We also see that the theoretical values are very close to the empirical values for all methods. And lastly we see that when we rank the observations, Spearmans rho gives the same value as when we didn't rank the values. 
```{r Exercise 2.1, echo = TRUE, include = TRUE}
myCop.norm <- ellipCopula(family="normal", dim=3, dispstr="ex", param = 0.4)
x1 <- rCopula(500, myCop.norm)
scatterplot3d(x1)
#Correlation
(rho1 = cor(x1[,1], x1[,2]))
(rho2 = cor(x1[,1], x1[,3]))
(rho3 = cor(x1[,2], x1[,3]))

#Kendall
(cor(x1[,1], x1[,2], method="kendall")) 
(cor(x1[,1], x1[,3], method="kendall"))
(cor(x1[,2], x1[,3], method="kendall"))
#Theoretical
(t1_theo = 2/pi*asin(0.4)) 

#Spearman
(cor(x1[,1], x1[,2], method="spearman"))
(cor(x1[,1], x1[,3], method="spearman"))
(cor(x1[,2], x1[,3], method="spearman"))
#Theoretical
rho_s_theo = 6/pi*asin(0.2)

#Rho 
(rho_Ss1 = cor(rank(x1[,1]), rank(x1[,2])))
(rho_Ss2 = cor(rank(x1[,1]), rank(x1[,3])))
(rho_Ss3 = cor(rank(x1[,2]), rank(x1[,3])))

```
\newpage

\subsection{Exercise 2.2}
```{r Exercise 2.2a,echo = TRUE, include = TRUE}
myCop.t1 <- ellipCopula(family="t", dim=2, dispstr="ex", param = -0.4, df = 8)
myCop.t2 <- ellipCopula(family="t", dim=2, dispstr="ex", param = 0, df = 8)
myCop.t3 <- ellipCopula(family="t", dim=2, dispstr="ex", param = 0.4, df = 8)

x2 = rCopula(500, myCop.t1)
x3 = rCopula(500, myCop.t2)
x4 = rCopula(500, myCop.t3)
```


As in previously when Gaussian copula was used, the methods seem to give results that are in accordance with the theoretical results. 
``` {r Exercise 2.2b, echo = TRUE, include = TRUE}
plot(x2)
plot(x3)
plot(x4)

(rho1 = cor(x2[,1], x2[,2]))
(rho2 = cor(x3[,1], x3[,2]))
(rho3 = cor(x4[,1], x4[,2]))

#Kendall

#Empirical
(cor(x2[,1], x2[,2], method="kendall")) 
(cor(x3[,1], x3[,2], method="kendall"))
(cor(x4[,1], x4[,2], method="kendall"))
#Theoretical for the three cases
(t1_theo = 2/pi*asin(-0.4)) 
(t1_theo = 2/pi*asin(0)) 
(t1_theo = 2/pi*asin(0.4)) 

#Spearman
#Empirical
(cor(x2[,1], x2[,2], method="spearman")) 
(cor(x3[,1], x3[,2], method="spearman"))
(cor(x4[,1], x4[,2], method="spearman"))
#Theoretical
(rho_s_theo = 6/pi*asin(-0.2))
(rho_s_theo = 6/pi*asin(0))
(rho_s_theo = 6/pi*asin(0.2))

```
\subsection{Exercise 2.3}
By looking at the appendix and solving for the expression for the $\tau$ using $\theta$ = 3 and k = 1. Using this, we get a value for $\tau$ thats about 0.3, which is in agreement with what is calculated below. 
```{r Exercise 2.3,echo = TRUE, include = TRUE}
frank.cop <- frankCopula(dim = 3, param=3)
x5 = rCopula(500, frank.cop)

(rho1_x5 = cor(x5[,1], x5[,2]))
(rho2_x5 = cor(x5[,1], x5[,3]))
(rho3_x5 = cor(x5[,2], x5[,3]))

(t1_x5 = cor(x5[,1], x5[,2], method = "kendall"))
(t2_x5 = cor(x5[,1], x5[,3], method = "kendall"))
(t3_x5 = cor(x5[,2], x5[,3], method = "kendall"))

(rho_s1 = cor(x5[,1], x5[,2], method = "kendall"))
(rho_s2 = cor(x5[,1], x5[,3], method = "kendall"))
(rho_s3 =cor(x5[,2], x5[,3], method = "kendall"))

```
\subsection{Exercise 2.4}
Below is the code for the function:
```{r Exercise 2.4, echo = TRUE, include = TRUE}
gammaRand <- function(n,theta){
   B= 1/theta
   M=runif(n)
   E = rexp(2*n)
   dim(E) = c(2,n)
   E=t(E)
   U = E/M
   sample = B/(U+B)
  
}
gammaRand(500, 0.5)
gammaRand(500, 1)
gammaRand(500, 5)
gammaRand(500, 50)

```
\subsection{Exercise 2.5}
The reason T(x) is increasing is due to the fact that arctanh is increasing in this interval (between -1 and 1). 
We get -1 and 1 as dependace for Kendalls Tau and Spearman, but only -0.914 and + 0.914 for normal correlation. Since Z and Y are created directly from X, it is reasonable that they should have perfect dependance. How ever, since there is a non-linear transform, normal Rho wont be quite as good when trying to measure this depndance. 

```{r Exercise 2.5,echo = TRUE, include = TRUE}
X = runif(1000)
Y= atanh(X)
Z=-Y
par(mfrow=c(2,1))
plot(X,Y)
plot(X,Z)
cor(X,Y, method="kendall")
cor(X,Z, method = "kendall")

cor(X,Y, method= "spearman")
cor(X,Z, method = "spearman")

cor(X,Y)
cor(X,Z)

```
\section{Exercise 3}
The plots for the t-copula. For the first plots (scatter, cdf, pdf) we can see that the plots show more dependance, which is reasonable since this is how they were created. 
```{r Exercise 3,echo = TRUE, include = TRUE}
par(mfrow= c(1,3)) #mfrow = c(1,2) is one row two cols.


plot(x3)
persp(myCop.t1, pCopula)
contour(myCop.t1, dCopula)


plot(x4)
persp(myCop.t2, pCopula)
contour(myCop.t2, dCopula)
```
\section{Exercise 4}
\subsection{Scatterplots}
The plots are, in order, Gaussian, Gumble, Clayton and t-copula. We can see that there is a somewhat similar dependance in all of the plots. We can see in plot 3 (Clayton), it seems to have quite a strong lower-tail dependance. The t-copula seems to have stronger tail dependance than the Gaussian, which might be expected. The Gumble copula seems to have slightly stronger upper tail dependance.
```{r Exercise 4,echo = TRUE, include = TRUE}
# Copulas
gauss.cop <- ellipCopula(family="normal", dim=2, dispstr="ex", param = 0.7) 
gumbel.cop <- archmCopula("gumbel",2 )
clayton.cop <- claytonCopula(2.2, dim = 2)
myCop.t <- ellipCopula(family="t", dim=2, dispstr="ex", param = 0.71, df = 4)
#Samples
x6= rCopula(2000, gauss.cop)
x7= rCopula(2000, gumbel.cop)
x8= rCopula(2000, clayton.cop)
x9= rCopula(2000, myCop.t)

#Plots
par(mfrow= c(2,2), mar = c(2,2,1,1), oma=c(1,1,0,0), mgp = c(2,1,0))
plot(x6)
plot(x7)
plot(x8)
plot(x9)

```
\newpage
\subsection{Quantile transformation}
What we can see in these plots is that we still have the same behaviour which was described in the previous section, only slightly clearer. 
```{r Quantile transformation,echo = TRUE, include = TRUE}
par(mfrow= c(2,2), mar = c(2,2,1,1), oma=c(1,1,0,0), mgp = c(2,1,0))
x6= rCopula(2000, gauss.cop)
x7= rCopula(2000, gumbel.cop)
x8= rCopula(2000, clayton.cop)
x9= rCopula(2000, myCop.t)

x6[,1] = rank(x6[,1])/length(x6[,1])
x6[,2] = rank(x6[,2])/length(x6[,2])

x6[,1] <- qnorm(x6[,1])
x6[,2] <- qnorm(x6[,2])



x7[,1] = rank(x7[,1])/length(x7[,1])
x7[,2] = rank(x7[,2])/length(x7[,2])

x7[,1] <- qnorm(x7[,1])
x7[,2] <- qnorm(x7[,2])


x8[,1] = rank(x8[,1])/length(x8[,1])
x8[,2] = rank(x8[,2])/length(x8[,2])

x8[,1] <- qnorm(x8[,1])
x8[,2] <- qnorm(x8[,2])

x9[,1] = rank(x9[,1])/length(x9[,1])
x9[,2] = rank(x9[,2])/length(x9[,2])

x9[,1] <- qnorm(x9[,1])
x9[,2] <- qnorm(x9[,2])
plot(x6)
plot(x7)
plot(x8)
plot(x9)

```
\section{Exercise 5}

As we can see, when we use the Gumbel copula with either two exponential margins or two standard normal margins, we get a similar Spearman Rho (around 0.65 to 0.75). 

```{r Exercise 5,echo = TRUE, include = TRUE}
cop_dist1 <- mvdc(copula=ellipCopula(family="norm", dim = 2, param=0.75), margins=c("norm", "exp"), paramMargins = list(list(mean=0,sd=2), list(rate=2))) 
cop_dist2 <- mvdc(copula=archmCopula("gumbel",dim=2, 2), margins=c("exp", "exp"), paramMargins = list(list(rate=4), list(rate=2)))

x10 = rMvdc(500, cop_dist1)
x11 = rMvdc(500, cop_dist2)
# First copula
cor(x10[,1],x10[,2], method = "spearman")
cor(x10[,1],x10[,2])

#Second copula
cor(x11[,1],x11[,2])
cor(x11[,1],x11[,2], method = "spearman")

#New margins to compare with second copula
cop_dist3 <- mvdc(copula=archmCopula("gumbel",dim=2, 2), margins=c("norm", "norm"), paramMargins = list(list(mean=0,sd=1), list(mean=0,sd=1)))
x12 = rMvdc(500, cop_dist3)
cor(x12[,1],x12[,2])
cor(x12[,1],x12[,2], method = "spearman")

```
\newpage
\section{Exercise 6}
\subsection*{Plots}
```{r exercise 6,echo = TRUE, include = TRUE}
par(mfrow= c(3,2))
plot(x10)
contour(cop_dist1, dMvdc, xlim=c(-6, 4), ylim=c(-0.1, 1.5))
plot(x11)
contour(cop_dist2, dMvdc, xlim = c(-0,0.35), ylim=c(-0,0.5))
plot(x12)
contour(cop_dist3, dMvdc, c(-2,2), ylim=c(-2,2))
```

\newpage
\section{Exercise 7}
We can see that the correlation of price and horsepower is high and positive, so it does seen to be the case that vehicles with higher horsepower are more costly. Simliarly, MPGcity and price have a negative correlation. 
```{r Exercise 7 car data,echo = TRUE, include = TRUE}
car_data <- read.csv("C:/Users/anton/Desktop/University/FMSN65  MASM33 - Quantitative Risk Management Using Copulas/Projects/Ass. 2/cardata.txt")

#Pairwise scatterplot of the data
pairs(car_data)
#Spearman
cor(car_data, method = "spearman")
cor.test(car_data[,1],car_data[,3], method = "spearman")
cor.test(car_data[,1],car_data[,2], method = "spearman")

```
\newpage
\section{Exercise 8}
We see that the probabilites of exceeding both thresholds is around 6-8 %, when using the 95th-percentile and 1-2 % for the 99th-percentile. We can also see that the ratios don't change when we change the threshold. We also see that Gumble copula gives the highest value for expected loss at 0.395, and the t-copula gives the lowest expected value at around 0.354. \newline
We can also see from the tail-dependance plots that the Gauss copula shows quite poor dependance. But the Gumble and t-copula shows some upper tail dependance. This makes it more reasonable to use these two copulas if what you are interested in is in the tails of a distribution. An example might be when dealing with stocks. 
```{r Exercise 8,echo = TRUE, include = TRUE}
Gauss <- mvdc( ellipCopula(family="normal", dim=2, dispstr="ex", param = 0.7), margins=c("lnorm", "lnorm"), paramMargins = list(list(mean=0, sd=1), list(mean=0, sd=1)))

t_cop = mvdc( ellipCopula(family="t", dim=2, dispstr="ex", param = 0.7, df = 2), margins=c("lnorm", "lnorm"), paramMargins = list(list(mean=0, sd=1), list(mean=0, sd=1)))

gumb = mvdc( archmCopula("gumbel" , 2), margins=c("lnorm", "lnorm"), paramMargins = list(list(mean=0, sd=1), list(mean=0, sd=1)))



#The threshould is:
(u = qlnorm(0.95,0,1))
(prob_gauss = 1- pMvdc(c(u,u), Gauss))
(prob_t = 1- pMvdc(c(u,u), t_cop))
(prob_gumb =1- pMvdc(c(u,u), gumb))

(r1 = prob_gauss/prob_t)
(r2 = prob_gauss/prob_gumb)
(r3 = prob_t/prob_gumb)
#The 99th percentile is:
(u = qlnorm(0.99,0,1))

#The probabilites of exceeding both thresholds:
(prob_gauss.a = 1- pMvdc(c(u,u), Gauss))
(prob_t.b = 1- pMvdc(c(u,u), t_cop))
(prob_gumb.c =1- pMvdc(c(u,u), gumb))

(r1a = prob_gauss/prob_t)
(r2b = prob_gauss/prob_gumb)
(r3c = prob_t/prob_gumb)

#t-copula
loss = rMvdc(10000, t_cop)
loss[which(loss[,1] <= u & loss[,2] <= u), ] <- 0
loss = rowSums(loss)
(E.t = mean(loss))

#Gauss
loss = rMvdc(10000, Gauss)
loss[which(loss[,1] <= u & loss[,2] <= u), ] <- 0
loss = rowSums(loss)
(E.Gauss = mean(loss))

#Gumble
loss = rMvdc(10000, gumb)
loss[which(loss[,1] <= u & loss[,2] <= u), ] <- 0
loss = rowSums(loss)
(E.gumb = mean(loss))


par(mfrow= c(1,3))
u = qlnorm(0.95,0,1)

loss = rMvdc(100000, Gauss)
tail.gauss = loss[apply(loss > u, 1, all), ]
plot(tail.gauss,ylim=c(0,100), xlim=c(0,100))

loss = rMvdc(100000, gumb)
tail.gumb = loss[apply(loss > u, 1, all), ]
plot(tail.gumb,ylim=c(0,100), xlim=c(0,100))

loss = rMvdc(100000, t_cop)
tail.t_cop = loss[apply(loss > u, 1, all), ]
plot(tail.t_cop,ylim=c(0,100), xlim=c(0,100))
```





```{r Exercise 9}
dat <- read.csv("C:/Users/anton/Desktop/University/FMSN65  MASM33 - Quantitative Risk Management Using Copulas/Projects/Ass. 2/insurance.txt",sep="")
#Skapar dist där jag använder gumbel copula och några lnorm marginaler.
mvdc1 =  mvdc(archmCopula("gumbel",5), margins=c("lnorm", "lnorm"), paramMargins=list(list(mean=0,sd=1), list(mean=0,sd=1)))
dat=as.matrix(dat)
# Hittar ML estimates för enbart margins, kommer användas vid någon tidspunkt. Tyckte "lnorm" passade marginsen rätt bra när man plottade hist av data. 
margin1Fit = mledist(dat[,1], "lnorm")
margin1Fit$estimate
margin2Fit = mledist(dat[,2], "lnorm")
margin2Fit$estimate

#Sätter några startvärden (det sista margins parametrar först, copulans paramter efter.)
start=c(9,1.5,8.5,1.4,5)
#FML fit. Alla parametrar av min dist fittas direkt. (går inte om jag använder marginfitsen ovan som startpunkt, av någon konstig anledning)
fit1 = fitMvdc(dat, mvdc1, start = start, optim.control=list(trace=TRUE, maxit = 2000))

#Testar även en Frank copula, med samma margins
mvdc2 =  mvdc(archmCopula("frank",5), margins=c("lnorm", "lnorm"), paramMargins=list(list(mean=0,sd=1), list(mean=0,sd=1)))
fit2 = fitMvdc(as.matrix(dat), mvdc2, start = start, optim.control=list(trace=TRUE, maxit = 2000))

#Testar clayton copula
mvdc3 =  mvdc(archmCopula("clayton",5), margins=c("lnorm", "lnorm"), paramMargins=list(list(mean=0,sd=1), list(mean=0,sd=1)))
fit3 = fitMvdc(as.matrix(dat), mvdc3, start = start, optim.control=list(trace=TRUE, maxit = 2000))




#Now lets try IFM

#First
loglik.marg <- function(b, x) sum(dlnorm(x, meanlog = b[1], sdlog = b[2],log = TRUE))
ctrl <- list(fnscale = -1)
b1hat <- optim(start[1:2], fn = loglik.marg, x = dat[, 1], control = ctrl)$par
b2hat <- optim(start[3:4], fn = loglik.marg, x = dat[, 2], control = ctrl)$par

udat <- cbind(plnorm(dat[, 1], meanlog = b1hat[1], sdlog= b1hat[2]), plnorm(dat[, 2], meanlog =  b2hat[1], sdlog = b2hat[2]))
fit.ifl1 <- fitCopula(data=udat, mvdc1@copula, start = 5)

#SECIBD

fit.ifl2 <- fitCopula(data=udat, mvdc2@copula, start = c(5))

#THIRD

fit.ifl3 <- fitCopula(data=udat, mvdc3@copula, start = 0.5)


#CML
n=length(dat[,1])
eu <- cbind((rank(dat[, 1]) - 0.5)/n, (rank(dat[, 2]) - 0.5)/n)

fit.cml1 <- fitCopula(data = eu, mvdc1@copula, start = 5)
fit.cml2 <- fitCopula(data = eu, mvdc2@copula, start = 5)
fit.cml3 <- fitCopula(data= eu, mvdc3@copula, start = 0.2)






#Kör goodness of fit. Hur ska man tolka det. 

#FML
gof1 = gofCopula(fit1@mvdc@copula, as.matrix(dat))
gof2 = gofCopula(fit2@mvdc@copula, as.matrix(dat))
gof3 = gofCopula(fit3@mvdc@copula, as.matrix(dat))
```


```{r 9 second part}
#Skapa function från exercise

RP = function(X, R , L){
  X1=X[1]
  X2=X[2]
  if (X1 < R) {
    val = 0}
  else if (X1 >= R && X1 < L){
    val = X1 - R + X2*(X1-R)/X1}
  else if ( X1 >= L){
    val=L - R + X2*(L-R)/L
  }
  return(val)
}
#Skapa lite samples
sample1 = rMvdc(10000, fit2@mvdc)
R_list = c(0, 2500, 7500, 9500, 0, 125000,375000, 475000, 0,250000,750000,950000)
L_list = c(10000, 500000, 1000000)
mean_list = c()
count =0
for (l in L_list) {
  count = count + 1
  ind_list = c((4*count-3),4*count)
  R_list1 = R_list[ind_list[1]:ind_list[2]]

  for (r in R_list1){
    val_list = c()
    for (i in 1:10000) {
      val_list = c(val_list, RP(sample1[i,], r, l))}
      print(min(val_list))
    mean_list = c(mean_list, mean(val_list))}}

plot(mean_list)
```

```{r 9 independent}
margin1Fit = mledist(dat[,1], "lnorm")
margin1Fit$estimate
margin2Fit = mledist(dat[,2], "lnorm")
margin2Fit$estimate
gen1 = c(rlnorm(10000, meanlog =margin1Fit$estimate[1], sdlog = margin1Fit$estimate[2]))
gen2 = c(rlnorm(10000, meanlog =margin2Fit$estimate[1], sdlog = margin2Fit$estimate[2]))
sample2 = matrix(c(gen1, gen2),ncol=2)





R_list = c(0, 2500, 7500, 9500, 0, 125000,375000, 475000, 0,250000,750000,950000)
L_list = c(10000, 500000, 1000000)
mean_list2 = c()
count =0
for (l in L_list) {
  count = count + 1
  ind_list = c((4*count-3),4*count)
  R_list1 = R_list[ind_list[1]:ind_list[2]]

  for (r in R_list1){
    val_list = c()
    for (i in 1:10000) {
      val_list = c(val_list, RP(sample2[i,], r, l))}
      print(min(val_list))
    mean_list2 = c(mean_list2, mean(val_list))}}

```








